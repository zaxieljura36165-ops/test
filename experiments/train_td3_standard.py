"""
æ ‡å‡†TD3è®­ç»ƒè„šæœ¬ï¼ˆéåˆ†å±‚ï¼Œä¸ä½¿ç”¨æƒ…å¢ƒæ„ŸçŸ¥å™ªå£°ä¸åŒç¼“å†²åŒºåˆ‡æ¢ï¼‰

ç‰¹ç‚¹ï¼š
1. ä»…ä½¿ç”¨å›ºå®šé«˜æ–¯æ¢ç´¢å™ªå£°ï¼ˆæ ‡å‡†TD3ï¼‰
2. ä¸ä½¿ç”¨æƒ…å¢ƒå™ªå£°æ§åˆ¶å™¨
3. è®­ç»ƒ/æ—¥å¿—/æ¨¡å‹è¾“å‡ºè·¯å¾„ä¸ä¸»ç®—æ³•åŒºåˆ†
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
import time
from datetime import datetime
import pickle

from configs.system_config import SystemConfig
from configs.td3_config import TD3Config
from src.algorithms.td3_standard import StandardTD3
from src.environments.simulation_env import SimulationEnvironment


def train_td3_standard(num_episodes: int = None, save_dir: str = 'results/models/td3_standard'):
    """
    è®­ç»ƒæ ‡å‡†TD3ç®—æ³•ï¼ˆéåˆ†å±‚ï¼‰
    
    Args:
        num_episodes: è®­ç»ƒè½®æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®å€¼ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    """
    # é…ç½®
    system_config = SystemConfig()
    td3_config = TD3Config()
    
    if num_episodes is None:
        num_episodes = td3_config.TRAINING_CONFIG['num_episodes']
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs('results/logs/td3_standard', exist_ok=True)
    os.makedirs('results/plots/td3_standard', exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆå¤ç”¨ç°æœ‰ç¯å¢ƒï¼Œåªæ˜¯ç®—æ³•ä¸åŒï¼‰
    from configs.mappo_config import MAPPOConfig
    mappo_config = MAPPOConfig()
    env = SimulationEnvironment(system_config, mappo_config)
    
    # åˆ›å»ºæ ‡å‡†TD3æ™ºèƒ½ä½“ï¼ˆéåˆ†å±‚ï¼‰
    agent = StandardTD3(td3_config, system_config)
    
    print("=" * 60)
    print("æ ‡å‡†TD3è®­ç»ƒï¼ˆéåˆ†å±‚ï¼‰ - å›ºå®šé«˜æ–¯å™ªå£°")
    print("=" * 60)
    print(f"è®¾å¤‡: {td3_config.DEVICE}")
    print(f"è½¦è¾†æ•°é‡: {system_config.NUM_VEHICLES}")
    print(f"RSUæ•°é‡: {system_config.NUM_RSU}")
    print(f"è®­ç»ƒè½®æ•°: {num_episodes}")
    print(f"ç¼“å†²åŒºå¤§å°: {td3_config.BUFFER_CONFIG['buffer_size']}")
    print(f"æ‰¹å¤§å°: {td3_config.BUFFER_CONFIG['batch_size']}")
    print("=" * 60)
    
    # æ¢ç´¢å™ªå£°ï¼ˆçº¿æ€§è¡°å‡ï¼š0.6 -> 0.1ï¼Œå‰60%è½®æ¬¡ï¼‰
    noise_start = 0.6
    noise_end = 0.1
    decay_episodes = max(1, int(num_episodes * 0.6))
    
    # è®­ç»ƒç»Ÿè®¡
    training_stats = {
        'episode_rewards': [],
        'episode_lengths': [],
        'success_rates': [],
        'noise_phases': [],     # ç»´æŒå­—æ®µä¸€è‡´æ€§
        'noise_scales': [],     # ç»´æŒå­—æ®µä¸€è‡´æ€§
        'actor_losses': [],
        'critic_losses': [],
        'objective_values': [],
    }
    
    # è¿è¡Œçª—å£ç»Ÿè®¡
    window_size = 50
    reward_window = []
    success_window = []
    
    start_time = time.time()
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        episode_reward = 0.0
        episode_length = 0
        
        done = False
        
        # å½“å‰episodeå™ªå£°
        if episode < decay_episodes:
            progress = episode / decay_episodes
            exploration_noise = noise_start + (noise_end - noise_start) * progress
        else:
            exploration_noise = noise_end

        while not done:
            # ä¸ºæ¯ä¸ªè½¦è¾†é€‰æ‹©åŠ¨ä½œ
            actions = {}
            all_local_states = []
            
            for vehicle_id in range(system_config.NUM_VEHICLES):
                # è·å–è½¦è¾†å±€éƒ¨çŠ¶æ€
                local_state = env._get_vehicle_state(vehicle_id, state)
                all_local_states.append(local_state)
                
                # æ ‡å‡†TD3ï¼šä¸ä¼ å…¥æƒ…å¢ƒä¿¡æ¯ï¼Œä½¿ç”¨å›ºå®šé«˜æ–¯å™ªå£°
                action = agent.select_action(
                    local_state, 
                    vehicle_id,
                    deterministic=False,
                    noise_std=exploration_noise
                )
                actions[vehicle_id] = action
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, shared_rewards, done, info = env.step(actions)

            # ä¸‹ä¸€çŠ¶æ€ï¼ˆæ‰€æœ‰è½¦è¾†ï¼‰
            all_next_local_states = [
                env._get_vehicle_state(vid, next_state) for vid in range(system_config.NUM_VEHICLES)
            ]

            # è”åˆraw_action
            joint_raw_actions = [
                actions[vid].get('raw_action', np.zeros_like(actions[vid].get('raw_action', np.array([]))))
                for vid in range(system_config.NUM_VEHICLES)
            ]
            
            # å­˜å‚¨ç»éªŒï¼ˆæ¯ä¸ªè½¦è¾†ï¼‰
            for vehicle_id in range(system_config.NUM_VEHICLES):
                local_state = env._get_vehicle_state(vehicle_id, state)
                next_local_state = env._get_vehicle_state(vehicle_id, next_state)
                
                # æ ‡å‡†TD3ï¼šå…¨éƒ¨è§†ä¸ºæ¢ç´¢æ€§ç»éªŒ
                is_noisy = True
                
                v_reward = shared_rewards.get(vehicle_id, 0.0)
                
                # è¡¥å……è”åˆä¿¡æ¯ï¼ˆé›†ä¸­å¼Criticï¼‰
                action_payload = actions.get(vehicle_id, {}).copy()
                action_payload['agent_id'] = vehicle_id
                action_payload['joint_raw_actions'] = [
                    np.array(r).flatten() for r in joint_raw_actions
                ]
                action_payload['all_local_states'] = [
                    ls.detach().cpu().numpy() if hasattr(ls, 'detach') else np.array(ls)
                    for ls in all_local_states
                ]
                action_payload['all_next_local_states'] = [
                    ls.detach().cpu().numpy() if hasattr(ls, 'detach') else np.array(ls)
                    for ls in all_next_local_states
                ]
                
                experience = {
                    'local_state': local_state,
                    'action': action_payload,
                    'reward': v_reward,
                    'next_local_state': next_local_state,
                    'done': done,
                    'global_state': state['global_state'],
                    'next_global_state': next_state['global_state'],
                }
                agent.store_experience(experience, agent_id=vehicle_id, is_noisy=is_noisy)
            
            # å…¨å±€å¥–åŠ±è®°å½•ç”¨äºæ˜¾ç¤º
            global_reward = list(shared_rewards.values())[0] if shared_rewards else 0.0
            episode_reward += global_reward
            episode_length += 1
            state = next_state
            
            # æ›´æ–°ç½‘ç»œï¼šæ¯ 5 æ­¥æ›´æ–°ä¸€æ¬¡ï¼ˆä¸ä¸»ç®—æ³•å¯¹é½ï¼‰
            if episode_length % 5 == 0:
                agent.update(episode)
        
        # è®¡ç®—æˆåŠŸç‡
        task_stats = env.task_manager.get_task_statistics()
        is_success = task_stats['success_rate'] > 0.5
        
        # è·å–ä¼˜åŒ–ç›®æ ‡å€¼
        episode_stats = env.optimization_problem.get_optimization_summary(env.current_time_slot)
        training_stats['objective_values'].append(episode_stats['objective_value'])
        
        # è®°å½•ç»Ÿè®¡
        training_stats['episode_rewards'].append(episode_reward)
        training_stats['episode_lengths'].append(episode_length)
        training_stats['success_rates'].append(task_stats['success_rate'])
        
        # å™ªå£°ç»Ÿè®¡ï¼ˆå›ºå®šå€¼ï¼‰
        training_stats['noise_phases'].append(0)
        training_stats['noise_scales'].append(exploration_noise)
        
        # æ›´æ–°çª—å£
        reward_window.append(episode_reward)
        success_window.append(task_stats['success_rate'])
        if len(reward_window) > window_size:
            reward_window.pop(0)
            success_window.pop(0)
        
        # æ‰“å°è¿›åº¦
        if episode % 10 == 0:
            avg_reward = np.mean(reward_window)
            avg_success = np.mean(success_window)
            elapsed = time.time() - start_time
            
            print(f"Episode {episode:4d} | "
                  f"Reward: {episode_reward:8.2f} (Avg: {avg_reward:8.2f}) | "
                  f"Length: {episode_length:3d} | "
                  f"Success: {task_stats['success_rate']:.2%} (Avg: {avg_success:.2%}) | "
                  f"Noise: {exploration_noise:.3f} | "
                  f"Time: {elapsed/60:.1f}min")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if episode > 0 and episode % td3_config.TRAINING_CONFIG['save_frequency'] == 0:
            checkpoint_path = os.path.join(save_dir, f'checkpoint_episode_{episode}.pth')
            agent.save_model(checkpoint_path)
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            stats_path = os.path.join('results/logs/td3_standard', 'training_stats.pkl')
            with open(stats_path, 'wb') as f:
                pickle.dump(training_stats, f)
        
        # è¯„ä¼°
        if episode > 0 and episode % td3_config.TRAINING_CONFIG['eval_frequency'] == 0:
            eval_reward, eval_success = evaluate(env, agent, td3_config, system_config)
            print(f"  ğŸ“Š è¯„ä¼°ç»“æœ: Avg Reward = {eval_reward:.2f}, Avg Success = {eval_success:.2%}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(save_dir, f'final_model_episode_{num_episodes}.pth')
    agent.save_model(final_path)
    
    # æ€§èƒ½è¯„ä¼°ï¼ˆè¾“å‡ºåˆ°ç‹¬ç«‹ç›®å½•ï¼‰
    from src.utils.evaluator import PerformanceEvaluator
    evaluator = PerformanceEvaluator(results_dir='results/td3_standard_eval')
    print("\nå¼€å§‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ...")
    evaluator.evaluate_training_performance(training_stats, save_plots=True)
    evaluator.save_training_data(training_stats, filename=f"td3_standard_training_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
    stats_path = os.path.join('results/logs/td3_standard', 'training_stats.pkl')
    with open(stats_path, 'wb') as f:
        pickle.dump(training_stats, f)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(training_stats, 'results/plots/td3_standard')
    
    total_time = time.time() - start_time
    print("=" * 60)
    print(f"è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_path}")
    print("=" * 60)
    
    return agent, training_stats


def evaluate(env, agent, td3_config, system_config, num_episodes: int = 10):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    agent.set_eval()
    
    eval_rewards = []
    eval_success_rates = []
    
    for _ in range(num_episodes):
        state = env.reset()
        episode_reward = 0.0
        done = False
        
        while not done:
            actions = {}
            for vehicle_id in range(system_config.NUM_VEHICLES):
                local_state = env._get_vehicle_state(vehicle_id, state)
                # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œ
                action = agent.select_action(
                    local_state, vehicle_id,
                    deterministic=True,
                    noise_std=0.0
                )
                actions[vehicle_id] = action
            
            next_state, shared_rewards, done, info = env.step(actions)
            global_reward = list(shared_rewards.values())[0] if shared_rewards else 0.0
            episode_reward += global_reward
            state = next_state
        
        task_stats = env.task_manager.get_task_statistics()
        eval_rewards.append(episode_reward)
        eval_success_rates.append(task_stats['success_rate'])
    
    return np.mean(eval_rewards), np.mean(eval_success_rates)


def plot_training_curves(stats: dict, save_dir: str):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # å¥–åŠ±æ›²çº¿
        ax = axes[0, 0]
        ax.plot(stats['episode_rewards'], alpha=0.3, color='blue')
        window = 50
        if len(stats['episode_rewards']) >= window:
            smoothed = np.convolve(stats['episode_rewards'], np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(stats['episode_rewards'])), smoothed, color='blue', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Episode Reward')
        ax.grid(True, alpha=0.3)
        
        # æˆåŠŸç‡æ›²çº¿
        ax = axes[0, 1]
        ax.plot(stats['success_rates'], alpha=0.3, color='green')
        if len(stats['success_rates']) >= window:
            smoothed = np.convolve(stats['success_rates'], np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(stats['success_rates'])), smoothed, color='green', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Success Rate')
        ax.set_title('Task Success Rate')
        ax.grid(True, alpha=0.3)
        
        # Episodeé•¿åº¦
        ax = axes[0, 2]
        ax.plot(stats['episode_lengths'], alpha=0.5, color='orange')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length')
        ax.set_title('Episode Length')
        ax.grid(True, alpha=0.3)
        
        # å™ªå£°é˜¶æ®µï¼ˆå›ºå®šï¼‰
        ax = axes[1, 0]
        ax.plot(stats['noise_phases'], color='red', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Phase')
        ax.set_title('Noise Control Phase')
        ax.grid(True, alpha=0.3)
        
        # å™ªå£°å¼ºåº¦
        ax = axes[1, 1]
        ax.plot(stats['noise_scales'], color='purple', alpha=0.7)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Noise Scale')
        ax.set_title('Exploration Noise')
        ax.grid(True, alpha=0.3)
        
        # ç®—æ³•ä¿¡æ¯
        ax = axes[1, 2]
        ax.text(0.5, 0.5, 'Standard TD3\n\nNon-hierarchical',
                ha='center', va='center', fontsize=14, transform=ax.transAxes)
        ax.set_title('Algorithm Info')
        ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'td3_standard_training_curves.png'), dpi=150)
        plt.close()
        
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_dir}/td3_standard_training_curves.png")
        
    except ImportError:
        print("è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒæ ‡å‡†TD3ç®—æ³•ï¼ˆéåˆ†å±‚ï¼‰')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save_dir', type=str, default='results/models/td3_standard', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    train_td3_standard(num_episodes=args.episodes, save_dir=args.save_dir)

