"""
åˆ†å±‚TD3è®­ç»ƒè„šæœ¬
ç»“åˆæƒ…å¢ƒæ„ŸçŸ¥è‡ªé€‚åº”å™ªå£°æ§åˆ¶å™¨

ç”¨æ³•:
    python experiments/train_td3.py
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
import time
from datetime import datetime
import pickle
from collections import defaultdict

from configs.system_config import SystemConfig
from configs.td3_config import TD3Config
from src.algorithms.hierarchical_td3 import HierarchicalTD3
from src.algorithms.noise_controller import build_context_info_from_state
from src.environments.simulation_env import SimulationEnvironment


def train_td3(num_episodes: int = None, save_dir: str = 'results/models/td3'):
    """
    è®­ç»ƒåˆ†å±‚TD3ç®—æ³•
    
    Args:
        num_episodes: è®­ç»ƒè½®æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®å€¼ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    """
    # é…ç½®
    system_config = SystemConfig()
    td3_config = TD3Config()
    
    if num_episodes is None:
        num_episodes = td3_config.TRAINING_CONFIG['num_episodes']
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs('results/logs/td3', exist_ok=True)
    os.makedirs('results/plots/td3', exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆå¤ç”¨ç°æœ‰ç¯å¢ƒï¼Œåªæ˜¯ç®—æ³•ä¸åŒï¼‰
    from configs.mappo_config import MAPPOConfig
    mappo_config = MAPPOConfig()
    env = SimulationEnvironment(system_config, mappo_config)
    
    # åˆ›å»ºTD3æ™ºèƒ½ä½“
    agent = HierarchicalTD3(td3_config, system_config)
    
    print("=" * 60)
    print("åˆ†å±‚TD3è®­ç»ƒ - æƒ…å¢ƒæ„ŸçŸ¥è‡ªé€‚åº”å™ªå£°æ§åˆ¶")
    print("=" * 60)
    print(f"è®¾å¤‡: {td3_config.DEVICE}")
    print(f"è½¦è¾†æ•°é‡: {system_config.NUM_VEHICLES}")
    print(f"RSUæ•°é‡: {system_config.NUM_RSU}")
    print(f"è®­ç»ƒè½®æ•°: {num_episodes}")
    print(f"ç¼“å†²åŒºå¤§å°: {td3_config.BUFFER_CONFIG['buffer_size']}")
    print(f"æ‰¹å¤§å°: {td3_config.BUFFER_CONFIG['batch_size']}")
    print("=" * 60)
    
    # è®­ç»ƒç»Ÿè®¡
    training_stats = {
        'episode_rewards': [],
        'episode_lengths': [],
        'success_rates': [],
        'noise_phases': [],
        'noise_scales': [],
        'actor_losses': [],
        'critic_losses': [],
        'objective_values': [],  # æ–°å¢ï¼šè®°å½•ä¼˜åŒ–ç›®æ ‡å€¼
    }
    
    # è¿è¡Œçª—å£ç»Ÿè®¡
    window_size = 50
    reward_window = []
    success_window = []
    
    start_time = time.time()
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        episode_reward = 0.0
        episode_length = 0
        
        done = False
        
        # === åŒç¼“å†²åŒºæ ¸å¿ƒï¼šå†³å®šæœ¬å›åˆæ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œ ===
        # Phase 1: 100% æ¢ç´¢æ€§åŠ¨ä½œ
        # Phase 2: 80% æ¢ç´¢æ€§ + 20% ç¡®å®šæ€§
        # Phase 3: 60% æ¢ç´¢æ€§ + 40% ç¡®å®šæ€§
        current_phase = agent.noise_controller.phase
        if current_phase == 1:
            use_deterministic = False
        elif current_phase == 2:
            use_deterministic = np.random.random() < 0.2
        else:  # Phase 3
            use_deterministic = np.random.random() < 0.4
        
        while not done:
            # ä¸ºæ¯ä¸ªè½¦è¾†é€‰æ‹©åŠ¨ä½œ
            actions = {}
            
            for vehicle_id in range(system_config.NUM_VEHICLES):
                # è·å–è½¦è¾†å±€éƒ¨çŠ¶æ€
                local_state = env._get_vehicle_state(vehicle_id, state)
                
                # æ„å»ºæƒ…å¢ƒä¿¡æ¯
                context_info = build_context_info_from_state(
                    local_state,
                    task_info=_get_task_info(env, vehicle_id),
                    queue_manager=env.queue_manager,
                    comm_model=env.comm_model
                )
                
                # é€‰æ‹©åŠ¨ä½œ
                action = agent.select_action(
                    local_state, 
                    vehicle_id,
                    context_info=context_info,
                    deterministic=use_deterministic
                )
                actions[vehicle_id] = action
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, shared_rewards, done, info = env.step(actions)
            
            # å­˜å‚¨ç»éªŒï¼ˆæ¯ä¸ªè½¦è¾†ï¼‰
            for vehicle_id in range(system_config.NUM_VEHICLES):
                local_state = env._get_vehicle_state(vehicle_id, state)
                next_local_state = env._get_vehicle_state(vehicle_id, next_state)
                
                # å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ ‡è®°ç»éªŒç±»å‹
                is_noisy = not use_deterministic
                
                # å…³é”®ä¿®å¤ï¼šå¥–åŠ±åˆ†é…
                # è™½ç„¶ shared_rewards ç›®å‰æ˜¯å…¨å±€å¥–åŠ±ï¼Œä½†é€»è¾‘ä¸Šåº”è¯¥æŒ‰è½¦è¾† ID ç´¢å¼•
                # ä¸”ä¸ºäº†ç¨³å®šï¼Œå¯ä»¥å¯¹å¥–åŠ±è¿›è¡Œé€‚å½“ç¼©æ”¾
                v_reward = shared_rewards.get(vehicle_id, 0.0)
                
                experience = {
                    'local_state': local_state,
                    'action': actions.get(vehicle_id, {}),
                    'reward': v_reward,
                    'next_local_state': next_local_state,
                    'done': done,
                    'global_state': state['global_state'],
                    'next_global_state': next_state['global_state'],
                }
                agent.store_experience(experience, is_noisy=is_noisy)
            
            # å…¨å±€å¥–åŠ±è®°å½•ç”¨äºæ˜¾ç¤º
            global_reward = list(shared_rewards.values())[0] if shared_rewards else 0.0
            episode_reward += global_reward
            episode_length += 1
            state = next_state
            
            # æ›´æ–°ç½‘ç»œï¼šé™ä½æ›´æ–°é¢‘ç‡ï¼Œæ¯ 5 æ­¥æ›´æ–°ä¸€æ¬¡
            if episode_length % 5 == 0:
                agent.update(episode)
        
        # è®¡ç®—æˆåŠŸç‡
        task_stats = env.task_manager.get_task_statistics()
        is_success = task_stats['success_rate'] > 0.5
        
        # è·å–ä¼˜åŒ–ç›®æ ‡å€¼
        episode_stats = env.optimization_problem.get_optimization_summary(env.current_time_slot)
        training_stats['objective_values'].append(episode_stats['objective_value'])
        
        # æ›´æ–°å™ªå£°æ§åˆ¶å™¨
        agent.update_noise_controller(episode, is_success)
        
        # è®°å½•ç»Ÿè®¡
        training_stats['episode_rewards'].append(episode_reward)
        training_stats['episode_lengths'].append(episode_length)
        training_stats['success_rates'].append(task_stats['success_rate'])
        
        noise_stats = agent.noise_controller.get_stats()
        training_stats['noise_phases'].append(noise_stats['phase'])
        training_stats['noise_scales'].append(noise_stats['global_noise'])
        
        # æ›´æ–°çª—å£
        reward_window.append(episode_reward)
        success_window.append(task_stats['success_rate'])
        if len(reward_window) > window_size:
            reward_window.pop(0)
            success_window.pop(0)
        
        # æ‰“å°è¿›åº¦
        if episode % 10 == 0:
            avg_reward = np.mean(reward_window)
            avg_success = np.mean(success_window)
            elapsed = time.time() - start_time
            
            print(f"Episode {episode:4d} | "
                  f"Reward: {episode_reward:8.2f} (Avg: {avg_reward:8.2f}) | "
                  f"Length: {episode_length:3d} | "
                  f"Success: {task_stats['success_rate']:.2%} (Avg: {avg_success:.2%}) | "
                  f"Phase: {noise_stats['phase']} | "
                  f"Noise: {noise_stats['global_noise']:.3f} | "
                  f"Time: {elapsed/60:.1f}min")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if episode > 0 and episode % td3_config.TRAINING_CONFIG['save_frequency'] == 0:
            checkpoint_path = os.path.join(save_dir, f'checkpoint_episode_{episode}.pth')
            agent.save_model(checkpoint_path)
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            stats_path = os.path.join('results/logs/td3', 'training_stats.pkl')
            with open(stats_path, 'wb') as f:
                pickle.dump(training_stats, f)
        
        # è¯„ä¼°
        if episode > 0 and episode % td3_config.TRAINING_CONFIG['eval_frequency'] == 0:
            eval_reward, eval_success = evaluate(env, agent, td3_config, system_config)
            print(f"  ğŸ“Š è¯„ä¼°ç»“æœ: Avg Reward = {eval_reward:.2f}, Avg Success = {eval_success:.2%}")
            
            # è®°å½•è¯„ä¼°ç»“æœåˆ°æ—¥å¿—ï¼ˆå¯é€‰ï¼Œä¸ºäº†å¯¹é½ MAPPO æŠ¥å‘Šï¼‰
            with open('results/logs/td3/evaluation_log.txt', 'a') as f:
                f.write(f"Episode {episode}: Reward={eval_reward:.2f}, Success={eval_success:.2%}\n")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(save_dir, f'final_model_episode_{num_episodes}.pth')
    agent.save_model(final_path)
    
    # æ€§èƒ½è¯„ä¼°ï¼ˆæ–°å¢ï¼šè°ƒç”¨ evaluator ç”ŸæˆæŠ¥å‘Šï¼Œå¯¹é½ MAPPOï¼‰
    from src.utils.evaluator import PerformanceEvaluator
    evaluator = PerformanceEvaluator(results_dir='results/td3_eval')
    print("\nå¼€å§‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ...")
    evaluator.evaluate_training_performance(training_stats, save_plots=True)
    evaluator.save_training_data(training_stats, filename=f"td3_training_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
    stats_path = os.path.join('results/logs/td3', 'training_stats.pkl')
    with open(stats_path, 'wb') as f:
        pickle.dump(training_stats, f)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(training_stats, 'results/plots/td3')
    
    total_time = time.time() - start_time
    print("=" * 60)
    print(f"è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_path}")
    print("=" * 60)
    
    return agent, training_stats


def evaluate(env, agent, td3_config, system_config, num_episodes: int = 10):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    agent.networks.eval()
    
    eval_rewards = []
    eval_success_rates = []
    
    for _ in range(num_episodes):
        state = env.reset()
        episode_reward = 0.0
        done = False
        
        while not done:
            actions = {}
            for vehicle_id in range(system_config.NUM_VEHICLES):
                local_state = env._get_vehicle_state(vehicle_id, state)
                # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œ
                action = agent.select_action(
                    local_state, vehicle_id,
                    deterministic=True
                )
                actions[vehicle_id] = action
            
            next_state, shared_rewards, done, info = env.step(actions)
            global_reward = list(shared_rewards.values())[0] if shared_rewards else 0.0
            episode_reward += global_reward
            state = next_state
        
        task_stats = env.task_manager.get_task_statistics()
        eval_rewards.append(episode_reward)
        eval_success_rates.append(task_stats['success_rate'])
    
    return np.mean(eval_rewards), np.mean(eval_success_rates)


def _get_task_info(env, vehicle_id: int):
    """ä»ç¯å¢ƒè·å–ä»»åŠ¡ä¿¡æ¯"""
    task_info = {
        'priority': 0.5,
        'deadline': 100.0,
        'arrival_time': 0.0,
        'current_time': env.current_time_slot * env.system_config.TIME_SLOT_DURATION,
    }
    
    # å°è¯•è·å–æ´»è·ƒä»»åŠ¡ä¿¡æ¯
    if vehicle_id in env.task_manager.active_tasks:
        tasks = env.task_manager.active_tasks[vehicle_id]
        if tasks:
            task = tasks[0]  # å–ç¬¬ä¸€ä¸ªä»»åŠ¡
            task_info['priority'] = task.priority
            task_info['deadline'] = task.deadline
            task_info['arrival_time'] = task.arrival_time
    
    return task_info


def plot_training_curves(stats: dict, save_dir: str):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # å¥–åŠ±æ›²çº¿
        ax = axes[0, 0]
        ax.plot(stats['episode_rewards'], alpha=0.3, color='blue')
        window = 50
        if len(stats['episode_rewards']) >= window:
            smoothed = np.convolve(stats['episode_rewards'], np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(stats['episode_rewards'])), smoothed, color='blue', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Episode Reward')
        ax.grid(True, alpha=0.3)
        
        # æˆåŠŸç‡æ›²çº¿
        ax = axes[0, 1]
        ax.plot(stats['success_rates'], alpha=0.3, color='green')
        if len(stats['success_rates']) >= window:
            smoothed = np.convolve(stats['success_rates'], np.ones(window)/window, mode='valid')
            ax.plot(range(window-1, len(stats['success_rates'])), smoothed, color='green', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Success Rate')
        ax.set_title('Task Success Rate')
        ax.grid(True, alpha=0.3)
        
        # Episodeé•¿åº¦
        ax = axes[0, 2]
        ax.plot(stats['episode_lengths'], alpha=0.5, color='orange')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length')
        ax.set_title('Episode Length')
        ax.grid(True, alpha=0.3)
        
        # å™ªå£°é˜¶æ®µ
        ax = axes[1, 0]
        ax.plot(stats['noise_phases'], color='red', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Phase')
        ax.set_title('Noise Control Phase')
        ax.set_yticks([1, 2, 3])
        ax.grid(True, alpha=0.3)
        
        # å™ªå£°å¼ºåº¦
        ax = axes[1, 1]
        ax.plot(stats['noise_scales'], color='purple', alpha=0.7)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Noise Scale')
        ax.set_title('Global Noise Scale')
        ax.grid(True, alpha=0.3)
        
        # æƒ…å¢ƒç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
        ax = axes[1, 2]
        ax.text(0.5, 0.5, 'Context-Aware\nNoise Control\n\nTD3 + Hierarchical\nDecision Making',
                ha='center', va='center', fontsize=14, transform=ax.transAxes)
        ax.set_title('Algorithm Info')
        ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'td3_training_curves.png'), dpi=150)
        plt.close()
        
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_dir}/td3_training_curves.png")
        
    except ImportError:
        print("è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒåˆ†å±‚TD3ç®—æ³•')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save_dir', type=str, default='results/models/td3', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    train_td3(num_episodes=args.episodes, save_dir=args.save_dir)

